{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6720,
     "status": "ok",
     "timestamp": 1616243424236,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "6DsuKRxNHO-W",
    "outputId": "7151dd90-2f69-44ac-a366-63c45ff5927f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git\n",
      "  Cloning https://github.com/hukkelas/DSFD-Pytorch-Inference.git to /tmp/pip-req-build-jfxicyxi\n",
      "Requirement already satisfied: torch>=1.6 in /home/hangduong/anaconda3/lib/python3.7/site-packages (from face-detection==0.2.1) (1.8.0)\n",
      "Requirement already satisfied: torchvision>=0.3.0 in /home/hangduong/anaconda3/lib/python3.7/site-packages (from face-detection==0.2.1) (0.8.0)\n",
      "Requirement already satisfied: numpy in /home/hangduong/anaconda3/lib/python3.7/site-packages (from face-detection==0.2.1) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /home/hangduong/anaconda3/lib/python3.7/site-packages (from torch>=1.6->face-detection==0.2.1) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/hangduong/anaconda3/lib/python3.7/site-packages (from torchvision>=0.3.0->face-detection==0.2.1) (7.2.0)\n",
      "Building wheels for collected packages: face-detection\n",
      "  Building wheel for face-detection (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for face-detection: filename=face_detection-0.2.1-py3-none-any.whl size=29722 sha256=55b416868531637615560b928c1d64ccf3e8a1c3254891b2c0a5e967b2a9d6f0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3tc5ddi5/wheels/11/5d/8c/04ffb7a0ca5427f3e674703ea75ecb16542e94efcc46d6bc1b\n",
      "Successfully built face-detection\n",
      "Installing collected packages: face-detection\n",
      "Successfully installed face-detection-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 7877,
     "status": "ok",
     "timestamp": 1616243476956,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "m7NndBNt0UPh"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "import face_detection\n",
    "\n",
    "import fnmatch, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1616243500589,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "7a42-Jk8A4mw"
   },
   "outputs": [],
   "source": [
    "WORKING_DIR = \"./\"\n",
    "SRC_PATH = \"./Data\"\n",
    "DES_PATH = \"./TrainingSet\"\n",
    "MODEL_DIR = \"./Model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dMxYUO9P3amF"
   },
   "outputs": [],
   "source": [
    "def extractFrame (desPath, videoPath, label):\n",
    "  imagesFolder = os.path.join(desPath, label)\n",
    "  os.mkdir(imagesFolder)\n",
    "\n",
    "  cap= cv2.VideoCapture(videoPath)\n",
    "  i=0\n",
    "\n",
    "  c=1\n",
    "  while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret == False:\n",
    "      break\n",
    "    \n",
    "    if c%12==0:\n",
    "      i+=1\n",
    "      cv2.imwrite(os.path.join(imagesFolder, str(i)+'.jpg'),frame)\n",
    "\n",
    "    c+= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RHVCaLOMuIeU"
   },
   "outputs": [],
   "source": [
    "def extractFace(frame, required_size=(160, 160)):\n",
    "  \n",
    "  detector = face_detection.build_detector(\n",
    "  \"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n",
    "\n",
    "  # detect faces in the image\n",
    "  results = detector.detect(frame)\n",
    "\n",
    "  # extract the bounding box from the first face\n",
    "  if len(results) > 0:\n",
    "    x_min, y_min, x_max, y_max = results[0][:4]\n",
    "    \n",
    "    # extract the face\n",
    "    x_min = max(int(x_min), 0)\n",
    "    x_max = int(x_max)\n",
    "    y_min = max(int(y_min), 0)\n",
    "    y_max = int(y_max)\n",
    "\n",
    "    face = frame[y_min:y_max, x_min:x_max].copy()\n",
    "    face = cv2.resize(face, required_size)\n",
    "\n",
    "    return face\n",
    "\n",
    "  # i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "oZISxKyjbX9L"
   },
   "outputs": [],
   "source": [
    "def loadFaces (folderPath):\n",
    "  faces = []\n",
    "  filenames = os.listdir(folderPath)\n",
    "  for i in tf.range(len(filenames)):\n",
    "    img = cv2.imread(os.path.join(folderPath,filenames[i]))\n",
    "    \n",
    "    face = extractFace(img)\n",
    "    \n",
    "    if (face is not None):\n",
    "\t\t  # store\n",
    "      # print(filenames[i], face.shape)\n",
    "      faces.append(face)\n",
    "  faces = np.array(faces)\n",
    "  return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M2WjMrGovWob"
   },
   "outputs": [],
   "source": [
    "def loadDataset (path):\n",
    "  X = []\n",
    "  Y = []\n",
    "\n",
    "  desPath = DES_PATH\n",
    "  os.mkdir(desPath)\n",
    "\n",
    "  # Extract frames from video\n",
    "  LABELS = []\n",
    "  data = fnmatch.filter(os.listdir(path), '*.mp4')\n",
    "  for file in data:\n",
    "    label = file[:-4]\n",
    "    LABELS.append(label)\n",
    "    extractFrame (desPath, os.path.join(path, file), label)\n",
    "\n",
    "  # Extract faces from the frame\n",
    "  for l in LABELS:\n",
    "    imgPath = os.path.join(desPath, l)\n",
    "    faces = loadFaces(imgPath)\n",
    "    labels = [l]*len(faces)\n",
    "\n",
    "    # print(type(faces))\n",
    "    X.extend(faces)\n",
    "    Y.extend(labels)\n",
    "\n",
    "  X = np.array(X)\n",
    "  Y = np.array(Y) \n",
    "  # print(X.shape)\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovY2xPCi2W08"
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree(DES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6Jbdh746F9NR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://folk.ntnu.no/haakohu/WIDERFace_DSFD_RES152.pth\" to /home/hangduong/.cache/torch/hub/checkpoints/WIDERFace_DSFD_RES152.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434216cbf98b48b0a55969c9afe8f83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=481004605.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = loadDataset(SRC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1616197003515,
     "user": {
      "displayName": "Hang Duong",
      "photoUrl": "",
      "userId": "10931732189932817471"
     },
     "user_tz": 420
    },
    "id": "OspxpGf88Akf",
    "outputId": "4d34bd78-3bb4-4345-db41-ddfda26a1985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(601, 160, 160, 3)\n",
      "(601,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "c2FSO3WW3CER"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dPO2-_Vw2TzU"
   },
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(WORKING_DIR, 'faces-dataset.npz'), x_train, x_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0z3nT4nMorMI"
   },
   "outputs": [],
   "source": [
    "data = np.load(os.path.join(WORKING_DIR, 'faces-dataset.npz'))\n",
    "x_train, x_val, y_train, y_val = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9996,
     "status": "ok",
     "timestamp": 1616243564887,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "HuQ-p-U4DGFl",
    "outputId": "8cf986d5-b700-4a12-afad-de3c199e9a53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
      "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "# https://drive.google.com/file/d/1971Xk5RwedbudGgTIrGAL4F7Aifu7id1/view\n",
    "model = tf.keras.models.load_model(os.path.join(MODEL_DIR, 'facenet_keras.h5'))\n",
    "\n",
    "# summarize input and output shape\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1616243552764,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "UdPptiWM46s9"
   },
   "outputs": [],
   "source": [
    "def get_embedding(model, face_pixels):\n",
    "\t# scale pixel values\n",
    "\tface_pixels = face_pixels.astype('float32')\n",
    " \n",
    "\t# standardize pixel values across channels (global)\n",
    "\tmean, std = face_pixels.mean(), face_pixels.std()\n",
    "\tface_pixels = (face_pixels - mean) / std\n",
    "\n",
    "\t# transform face into one sample\n",
    "\tsamples = np.expand_dims(face_pixels, axis=0)\n",
    " \n",
    "\t# make prediction to get embedding\n",
    "\tyhat = model.predict(samples)\n",
    " \n",
    "\treturn yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64127,
     "status": "ok",
     "timestamp": 1616227654048,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "xyhx0TVU5YHv",
    "outputId": "ca221e81-40c0-4f64-a6e7-676ef566ba03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 128)\n",
      "(121, 128)\n"
     ]
    }
   ],
   "source": [
    "x_em_train = []\n",
    "\n",
    "for face_pixels in x_train:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tx_em_train.append(embedding)\n",
    "x_em_train = np.array(x_em_train)\n",
    "\n",
    "print(x_em_train.shape)\n",
    "\n",
    "# convert each face in the test set to an embedding\n",
    "x_em_val = []\n",
    "for face_pixels in x_val:\n",
    "\tembedding = get_embedding(model, face_pixels)\n",
    "\tx_em_val.append(embedding)\n",
    "x_em_val = np.array(x_em_val)\n",
    "\n",
    "print(x_em_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "y07X4STmo4cr"
   },
   "outputs": [],
   "source": [
    "# save arrays to one file in compressed format\n",
    "np.savez_compressed(os.path.join(WORKING_DIR,'faces-embeddings.npz'), x_em_train, x_em_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 1761,
     "status": "ok",
     "timestamp": 1616243507482,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "HTZVAQNdo5zB"
   },
   "outputs": [],
   "source": [
    "data = np.load(os.path.join(WORKING_DIR,\"faces-embeddings.npz\"))\n",
    "x_em_train, x_em_val, y_train, y_val = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1616243691317,
     "user": {
      "displayName": "hang duongngoc",
      "photoUrl": "",
      "userId": "03950882304783104405"
     },
     "user_tz": 420
    },
    "id": "INrz48un7ty8",
    "outputId": "a2bc746e-d7ff-4367-890d-3b3ce4cf3dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: train=98.542, val=96.694\n"
     ]
    }
   ],
   "source": [
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(x_em_train)\n",
    "valX = in_encoder.transform(x_em_val)\n",
    "\n",
    "# label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(y_train)\n",
    "trainY = out_encoder.transform(y_train)\n",
    "valY = out_encoder.transform(y_val)\n",
    "\n",
    "# fit model\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(trainX, trainY)\n",
    "# predict\n",
    "yhat_train = model.predict(trainX)\n",
    "yhat_test = model.predict(valX)\n",
    "\n",
    "# score\n",
    "score_train = accuracy_score(trainY, yhat_train)\n",
    "score_test = accuracy_score(valY, yhat_test)\n",
    "\n",
    "# summarize\n",
    "print('Accuracy: train=%.3f, val=%.3f' % (score_train*100, score_test*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Kiên (100.000)\n",
      "Expected: Kiên\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Tuấn (100.000)\n",
      "Expected: Tuấn\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Việt Đức (100.000)\n",
      "Expected: Việt Đức\n",
      "Predicted: Vân (100.000)\n",
      "Expected: Vân\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (99.638)\n",
      "Expected: Việt Đức\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Tuấn (92.994)\n",
      "Expected: Tuấn\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (96.911)\n",
      "Expected: Kiên\n",
      "Predicted: Thắng (97.314)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (100.000)\n",
      "Expected: Việt Đức\n",
      "Predicted: Kiên (19.854)\n",
      "Expected: Kiên\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Tân (100.000)\n",
      "Expected: Tân\n",
      "Predicted: Kiên (100.000)\n",
      "Expected: Kiên\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Tuấn (99.878)\n",
      "Expected: Tuấn\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Tuấn (98.314)\n",
      "Expected: Tuấn\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Tân (100.000)\n",
      "Expected: Tân\n",
      "Predicted: HĐức (100.000)\n",
      "Expected: HĐức\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Việt Đức (99.610)\n",
      "Expected: Việt Đức\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Kiên (100.000)\n",
      "Expected: Kiên\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Xuân Anh (97.525)\n",
      "Expected: Tuấn\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Vân (100.000)\n",
      "Expected: Vân\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Vân (99.480)\n",
      "Expected: Hùng\n",
      "Predicted: Kiên (31.289)\n",
      "Expected: Kiên\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (99.013)\n",
      "Expected: Việt Đức\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Tuấn (99.885)\n",
      "Expected: Tuấn\n",
      "Predicted: Tân (70.105)\n",
      "Expected: Tân\n",
      "Predicted: Tuấn (100.000)\n",
      "Expected: Tuấn\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Linh (21.864)\n",
      "Expected: Tuấn\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Kiên (99.995)\n",
      "Expected: Kiên\n",
      "Predicted: Kiên (100.000)\n",
      "Expected: Kiên\n",
      "Predicted: Vân (100.000)\n",
      "Expected: Vân\n",
      "Predicted: Linh (69.808)\n",
      "Expected: Kiên\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Việt Đức (83.823)\n",
      "Expected: Việt Đức\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Kiên (55.836)\n",
      "Expected: Kiên\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Kiên (100.000)\n",
      "Expected: Kiên\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (98.101)\n",
      "Expected: Việt Đức\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Vân (100.000)\n",
      "Expected: Vân\n",
      "Predicted: Tân (100.000)\n",
      "Expected: Tân\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Việt Đức (100.000)\n",
      "Expected: Việt Đức\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Kiên (99.996)\n",
      "Expected: Kiên\n",
      "Predicted: Vân (100.000)\n",
      "Expected: Vân\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Trường (100.000)\n",
      "Expected: Trường\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Vân (96.962)\n",
      "Expected: Hùng\n",
      "Predicted: Vân (98.718)\n",
      "Expected: Hùng\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Kiên (99.950)\n",
      "Expected: Kiên\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Hùng (100.000)\n",
      "Expected: Hùng\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Tân (100.000)\n",
      "Expected: Tân\n",
      "Predicted: Linh (100.000)\n",
      "Expected: Linh\n",
      "Predicted: Xuân Anh (100.000)\n",
      "Expected: Xuân Anh\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Xuân Anh (98.784)\n",
      "Expected: Tuấn\n",
      "Predicted: Quân (99.999)\n",
      "Expected: Quân\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Thắng (100.000)\n",
      "Expected: Thắng\n",
      "Predicted: Việt Đức (100.000)\n",
      "Expected: Việt Đức\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Đức (100.000)\n",
      "Expected: Đức\n",
      "Predicted: Tân (100.000)\n",
      "Expected: Tân\n",
      "Predicted: Quân (100.000)\n",
      "Expected: Quân\n",
      "Predicted: Hiếu (100.000)\n",
      "Expected: Hiếu\n",
      "Predicted: Tân (99.939)\n",
      "Expected: Tân\n"
     ]
    }
   ],
   "source": [
    "yhat_class =model.predict(x_em_val)\n",
    "yhat_prob = model.predict_proba(x_em_val)\n",
    "for i in range(len(x_em_val)):\n",
    "  classIdx = yhat_class[i]\n",
    "  classProb = yhat_prob[i, classIdx] * 100\n",
    "\n",
    "  predictNames = out_encoder.inverse_transform(yhat_class)\n",
    "  print('Predicted: %s (%.3f)' % (predictNames[i], classProb))\n",
    "  print('Expected: %s' % y_val[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FaceRecognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
